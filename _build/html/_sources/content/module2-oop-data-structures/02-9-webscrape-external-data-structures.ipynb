{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b376b6-f5a8-4a39-8f12-8ae27d7a0594",
   "metadata": {},
   "source": [
    "(module2-oop-data-structures/02-9-webscrape-external-data-structures)=\n",
    "# 9. WebScraping as External Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e2ea7-6d8d-476c-b9e3-2000c91a8366",
   "metadata": {},
   "source": [
    "#  Web Scraping and Databases: A Hybrid Approach\n",
    "While web scraping is often used to extract data for immediate use, it does not store it. A powerful workflow would be:\n",
    "- **Scrape data** from online sources.\n",
    "- **Store it in a structured database (SQL or NoSQL)** for long-term analysis.\n",
    "- **Query it later** instead of repeatedly scraping.\n",
    "\n",
    "\n",
    "## Web Scraping as a Data Acquisition Method\n",
    "Web scraping is a method of extracting **external data** from structured or semi-structured sources on the web and transforming it into a usable format. Unlike databases or file storage, web scraping **does not inherently store data**—it is a way to retrieve and structure data from the web dynamically. It allows access to **data stored in HTML web pages** that might not be available via an API.\n",
    "\n",
    "### Web Scraping vs. APIs\n",
    "| Feature         | Web Scraping | APIs |\n",
    "|---------------|-------------|------|\n",
    "| **Access** | Extracts data from web pages (HTML tables, text, lists) | Queries structured data from a service (often JSON or XML) |\n",
    "| **Structure** | Often semi-structured (needs parsing) | Well-structured |\n",
    "| **Reliability** | Pages may change, breaking the scraper | More stable (unless API changes) |\n",
    "| **Use Case** | Extracting tables, research data, metadata from articles | Accessing structured datasets (PubChem, NCBI, weather data) |\n",
    "\n",
    "Thus, **web scraping is an alternative to APIs when structured access is unavailable**.\n",
    "\n",
    "---\n",
    "\n",
    "## Web Scraping as a Bridge from Classical Literature to Structured Data\n",
    "Scientific data has historically been communicated through **journal articles, textbooks, and reports**. Many modern scientific knowledge repositories (e.g., Wikipedia, research databases) still store information in text-based formats rather than structured databases. Web scraping allows you to:\n",
    "\n",
    "- Extract **tabular data** (like chemical properties from Wikipedia or patents).\n",
    "- Retrieve **text-based metadata** (such as author names, abstracts, and citations).\n",
    "- Collect **non-tabular structured information** (like structured web pages with lists of elements).\n",
    "\n",
    "By applying **text parsing, table extraction, and structured storage**, web scraping allows researchers to **convert human-readable content into machine-readable data**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3f663f1-964f-448c-bff7-a981684e41d4",
   "metadata": {},
   "source": [
    "# 2. Setting up your env\n",
    "```bash\n",
    "conda activate your-webscrape-env\n",
    "conda install -c conda-forge beautifulsoup4 requests lxml\n",
    "```\n",
    "\n",
    "- `beautifulsoup4`: The core parsing library.\n",
    "- `requests`: To fetch web pages.\n",
    "- `lxml`: A fast and efficient HTML/XML parser (optional but recommended for performance).\n",
    "\n",
    "the image below shows how I set up a webscrape environment while also connecting the kernel to Jupyter lab runninig in the base env.\n",
    "\n",
    "\n",
    "![image.png](images/fc552991-8a5b-45ce-8133-7017720724df.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9803b6d-4870-426b-8e02-b72418ef79e0",
   "metadata": {},
   "source": [
    "Webscrape a wikipedia element box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3891d4a-3023-44d6-a2f1-1a216e0acaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to: /home/rebelford/data/wikipedia/carbon_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime  # <- make sure this is at the top of your script\n",
    "\n",
    "# Define the directory and file path\n",
    "wikipedia_dir = os.path.expanduser(\"~/data/wikipedia/\")\n",
    "os.makedirs(wikipedia_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "carbon_wikipedia_path = os.path.join(wikipedia_dir, \"carbon_data.json\")\n",
    "\n",
    "def scrape_infobox(element_name):\n",
    "    \"\"\"Scrapes the Wikipedia infobox for a given element and returns a dictionary of properties.\"\"\"\n",
    "    \n",
    "    url = f\"https://en.wikipedia.org/wiki/{element_name}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the Infobox element\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "\n",
    "    # Dictionary to store element properties\n",
    "    element_data = {}\n",
    "\n",
    "\n",
    "    # Loop through table rows and extract key-value pairs\n",
    "    for row in infobox.find_all('tr'):\n",
    "        header = row.find('th')\n",
    "        value = row.find('td')\n",
    "\n",
    "        if header and value:\n",
    "            key = header.get_text(strip=True)  # Get property name\n",
    "            val = value.get_text(\" \", strip=True)  # Extract value, keeping spaces\n",
    "            element_data[key] = val\n",
    "\n",
    "    # Add provenance metadata\n",
    "    element_data[\"_source\"] = {\n",
    "        \"Wikipedia_URL\": url,\n",
    "        \"Scraped_from\": \"Wikipedia Infobox Element\",\n",
    "        #\"Scraped_on\": requests.get(\"https://worldtimeapi.org/api/timezone/Etc/UTC\").json()['datetime']\n",
    "        \"Scraped_on\": datetime.utcnow().isoformat() + \"Z\"\n",
    "    }\n",
    "\n",
    "    return {element_name: element_data}\n",
    "\n",
    "# Scrape data for Hydrogen\n",
    "carbon_data = scrape_infobox(\"Carbon\")\n",
    "\n",
    "# Save JSON data to file\n",
    "with open(carbon_wikipedia_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(carbon_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data saved successfully to: {carbon_wikipedia_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b44a-271c-44a0-90e1-4afec88daf7a",
   "metadata": {},
   "source": [
    "Print out the available properties for hydrogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd896f85-a9f4-43a0-8b4e-2ab3105de13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties available for Hydrogen:\n",
      "- Appearance\n",
      "- \n",
      "- Atomic number(Z)\n",
      "- Group\n",
      "- Period\n",
      "- Block\n",
      "- Electron configuration\n",
      "- Electrons per shell\n",
      "- PhaseatSTP\n",
      "- Melting point\n",
      "- Boiling point\n",
      "- Density(at STP)\n",
      "- when liquid (atm.p.)\n",
      "- when liquid (atb.p.)\n",
      "- Triple point\n",
      "- Critical point\n",
      "- Heat of fusion\n",
      "- Heat of vaporization\n",
      "- Molar heat capacity\n",
      "- P(Pa)\n",
      "- atT(K)\n",
      "- Oxidation states\n",
      "- Electronegativity\n",
      "- Ionization energies\n",
      "- Covalent radius\n",
      "- Van der Waals radius\n",
      "- Natural occurrence\n",
      "- Crystal structure\n",
      "- Lattice constants\n",
      "- Thermal conductivity\n",
      "- Magnetic ordering\n",
      "- Molar magnetic susceptibility\n",
      "- Speed of sound\n",
      "- CAS Number\n",
      "- Discoveryand first isolation\n",
      "- Named by\n",
      "- Recognized as anelementby\n",
      "- Main isotopes\n",
      "- 1H\n",
      "- 2H\n",
      "- 3H\n",
      "- _source\n",
      "\n",
      "\n",
      "Named by: Property not found\n",
      "(Source: https://en.wikipedia.org/wiki/Hydrogen)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define file path\n",
    "wikipedia_dir = os.path.expanduser(\"~/data/wikipedia/\")\n",
    "hydrogen_wikipedia_path = os.path.join(wikipedia_dir, \"hydrogen_data.json\")\n",
    "\n",
    "# Load the JSON file\n",
    "with open(hydrogen_wikipedia_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    hydrogen_data = json.load(f)\n",
    "\n",
    "# Extract Hydrogen properties\n",
    "hydrogen_properties = hydrogen_data.get(\"Hydrogen\", {})\n",
    "\n",
    "# Print all keys (property names)\n",
    "print(\"Properties available for Hydrogen:\")\n",
    "for key in hydrogen_properties.keys():\n",
    "    print(\"-\", key)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Print value for a specific key (\"Named by\") with provenance\n",
    "key_to_lookup = \"Named by\"\n",
    "value = hydrogen_properties.get(key_to_lookup, \"Property not found\")\n",
    "\n",
    "source_url = hydrogen_properties.get(\"_source\", {}).get(\"Wikipedia_URL\", \"Unknown Source\")\n",
    "\n",
    "print(f\"{key_to_lookup}: {value}\")\n",
    "print(f\"(Source: {source_url})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2188b2-2fd9-4f0e-b890-e77bbbeb1ca7",
   "metadata": {},
   "source": [
    "Scrape the chem infoboxes of the halogens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e8c680-5e39-4d77-88f2-22fea6b943e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to: /home/rebelford/data/wikipedia/halogens_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Define the list of halogens (either names or symbols)\n",
    "halogens = [\"Fluorine\", \"Chlorine\", \"Bromine\", \"Iodine\", \"Astatine\"]\n",
    "\n",
    "# Define file path for saving the scraped data\n",
    "wikipedia_dir = os.path.expanduser(\"~/data/wikipedia/\")\n",
    "os.makedirs(wikipedia_dir, exist_ok=True)  # Ensure directory exists\n",
    "halogens_wikipedia_path = os.path.join(wikipedia_dir, \"halogens_data.json\")\n",
    "\n",
    "\n",
    "def scrape_infobox(element_name):\n",
    "    \"\"\"Scrapes the Wikipedia infobox for a given element and returns a dictionary of properties.\"\"\"\n",
    "    \n",
    "    url = f\"https://en.wikipedia.org/wiki/{element_name}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the Infobox element\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "\n",
    "    # Dictionary to store element properties\n",
    "    element_data = {}\n",
    "   \n",
    "\n",
    "    # Loop through table rows and extract key-value pairs\n",
    "    for row in infobox.find_all('tr'):\n",
    "        header = row.find('th')\n",
    "        value = row.find('td')\n",
    "\n",
    "        if header and value:\n",
    "            key = header.get_text(strip=True)  # Get property name\n",
    "            val = value.get_text(\" \", strip=True)  # Extract value, preserving spaces\n",
    "            element_data[key] = val\n",
    "\n",
    "    # Add provenance metadata\n",
    "    element_data[\"_source\"] = {\n",
    "        \"Wikipedia_URL\": url,\n",
    "        \"Scraped_from\": \"Wikipedia Infobox Element\",\n",
    "        \"Scraped_on\": datetime.now(timezone.utc).isoformat()  # UTC time with explicit timezone\n",
    "    }\n",
    "  \n",
    "    return {element_name: element_data}\n",
    "\n",
    "\n",
    "# Create dictionary of dictionaries for all halogens\n",
    "halogens_data = {}\n",
    "\n",
    "for element in halogens:\n",
    "    halogens_data.update(scrape_infobox(element))\n",
    "\n",
    "# Save JSON data to file\n",
    "with open(halogens_wikipedia_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(halogens_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data saved successfully to: {halogens_wikipedia_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6717c94-3b8f-45bf-a54c-81dad1f72710",
   "metadata": {},
   "source": [
    "Now go to user-home/data/wikipedia/ and open the json file for halogens and explore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [webscrape]",
   "language": "python",
   "name": "webscrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
